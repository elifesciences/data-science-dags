{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "project_id = 'elife-data-pipeline'\n",
    "source_dataset = 'de_dev'\n",
    "output_dataset = 'de_dev'\n",
    "output_table_prefix = 'data_science_'\n",
    "manuscript_min_tf = 10\n",
    "manuscript_max_tf = 0.9\n",
    "state_path = 's3://ci-elife-data-pipeline/airflow-config/data-science/state-dev'"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "parameters"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from typing import List, Tuple, TypeVar\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import data_science_pipeline.configure_warnings  # pylint: disable=unused-import\n",
    "import data_science_pipeline.configure_notebook_logging  # pylint: disable=unused-import\n",
    "\n",
    "from data_science_pipeline.sql import get_sql\n",
    "from data_science_pipeline.utils.bq import to_gbq\n",
    "from data_science_pipeline.utils.io import load_object_from\n",
    "from data_science_pipeline.utils.jupyter import (\n",
    "    read_big_query as _read_big_query\n",
    ")\n",
    "from data_science_pipeline.peerscout.models import (\n",
    "    WeightedKeywordModel\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-15551155a408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import pandas as pd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_science_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_warnings\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_path = os.path.join(state_path, 'reviewing_editor_model.joblib')\n",
    "recommendation_output_table_name = '{output_dataset}.{prefix}{suffix}'.format(\n",
    "    output_dataset=output_dataset,\n",
    "    prefix=output_table_prefix,\n",
    "    suffix='reviewing_editor_recommendation'\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('loading model from:', model_path)\n",
    "model_dict = load_object_from(model_path)\n",
    "model_dict.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "editor_tf_idf_vectorizer = model_dict['editor_tf_idf_vectorizer']\n",
    "editor_tf_idf_vectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "editor_tf_idf = model_dict['editor_tf_idf']\n",
    "editor_tf_idf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "editor_names = model_dict['editor_names']\n",
    "# editor_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "editor_person_ids = model_dict['editor_person_ids']\n",
    "# editor_person_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "editor_person_id_by_name_map = dict(zip(editor_names, editor_person_ids))\n",
    "# editor_person_id_by_name_map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weighted_keyword_valid_model = WeightedKeywordModel.from_tf_matrix(\n",
    "    editor_tf_idf.todense(),\n",
    "    vectorizer=editor_tf_idf_vectorizer,\n",
    "    choices=editor_names\n",
    ")\n",
    "# weighted_keyword_valid_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "read_big_query = partial(_read_big_query, project_id=project_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "default_query_props = dict(project=project_id, dataset=source_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we are using the same manuscript list used for senior editor recommendation\n",
    "# this is because we want to recommend reviewing editors to consult with (not for assignment)\n",
    "manuscript_version_for_recommendation_df = read_big_query(\n",
    "    get_sql('manuscript-version-initial-submissions-for-senior-editor-recommendation.sql').format(\n",
    "        **default_query_props\n",
    "    )\n",
    ")\n",
    "# print(len(manuscript_version_for_recommendation_df))\n",
    "# manuscript_version_for_recommendation_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keyword_similarity = cosine_similarity(\n",
    "    editor_tf_idf_vectorizer.transform(\n",
    "        manuscript_version_for_recommendation_df\n",
    "        ['extracted_keywords']\n",
    "    ),\n",
    "    editor_tf_idf\n",
    ")\n",
    "# print(keyword_similarity.max())\n",
    "# keyword_similarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weighted_keyword_valid_model.predict_ranking(\n",
    "    manuscript_version_for_recommendation_df['extracted_keywords'][:1],\n",
    ").proba_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "manuscript_matching_keywords_list = weighted_keyword_valid_model.predict_ranking(\n",
    "    manuscript_version_for_recommendation_df['extracted_keywords']\n",
    ").matching_keywords_list\n",
    "pd.Series(manuscript_matching_keywords_list[:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "T = TypeVar('T')\n",
    "\n",
    "\n",
    "def get_recommended_editors_with_probability(\n",
    "        proba_matrix: List[List[float]],\n",
    "        manuscript_matching_keywords_list: List[List[Tuple[float, str]]],\n",
    "        indices: List[T],\n",
    "        threshold: float = 0.5) -> List[List[Tuple[float, T]]]:\n",
    "    return [\n",
    "        sorted([\n",
    "            (\n",
    "                p,\n",
    "                key,\n",
    "                sum(\n",
    "                    s for s, _ in editor_matching_keywords\n",
    "                ),\n",
    "                editor_matching_keywords\n",
    "            )\n",
    "            for p, key, editor_matching_keywords in zip(\n",
    "                row,\n",
    "                indices,\n",
    "                editors_matching_keywords\n",
    "            ) if p >= threshold\n",
    "        ], reverse=True)\n",
    "        for row, editors_matching_keywords in zip(proba_matrix, manuscript_matching_keywords_list)\n",
    "    ]\n",
    "\n",
    "\n",
    "prediction_results_with_similarity = pd.Series(\n",
    "    get_recommended_editors_with_probability(\n",
    "        keyword_similarity,\n",
    "        manuscript_matching_keywords_list,\n",
    "        editor_names,\n",
    "        threshold=0.001\n",
    "    ),\n",
    "    index=manuscript_version_for_recommendation_df.index\n",
    ")\n",
    "# prediction_results_with_similarity[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_results_df = pd.concat([\n",
    "    manuscript_version_for_recommendation_df['version_id'],\n",
    "    prediction_results_with_similarity.to_frame('prediction'),\n",
    "], axis=1)\n",
    "# print(len(prediction_results_df))\n",
    "# prediction_results_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prediction_results_df['prediction'][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_results_flat_df = pd.DataFrame([\n",
    "    {\n",
    "        'version_id': row.version_id,\n",
    "        'score': predicted_editor[0],\n",
    "        'name': predicted_editor[1],\n",
    "        'person_id': editor_person_id_by_name_map[predicted_editor[1]],\n",
    "        'matching_keyword_score': predicted_editor[2],\n",
    "        'matching_keywords': [{\n",
    "            'score': keyword_score,\n",
    "            'keyword': keyword\n",
    "        } for keyword_score, keyword in predicted_editor[3]],\n",
    "    }\n",
    "    for row in prediction_results_df.itertuples()\n",
    "    for predicted_editor in row.prediction\n",
    "])\n",
    "# print(len(prediction_results_flat_df))\n",
    "# prediction_results_flat_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_results_flat_df['version_id'].nunique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction_results_flat_df.max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('writing to:', recommendation_output_table_name)\n",
    "to_gbq(\n",
    "    prediction_results_flat_df,\n",
    "    recommendation_output_table_name,\n",
    "    project_id=project_id,\n",
    "    if_exists='replace'\n",
    ")\n",
    "print('done')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "34dc5cd04d546381f8b2e16e7d2b585cad685ef36d4aa934fdf52e04155ce140"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
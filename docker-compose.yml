version: '3.4'


x-pipeline-env:
  &pipeline-env
  - LOAD_EX=n
  - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
  - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/1
  - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  - AIRFLOW__CORE__FERNET_KEY='81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs='
  - AIRFLOW__WEBSERVER__SECRET_KEY='WmZHRmJwd1dCUEp6Xl4zVA=='
  - AIRFLOW__API__ENABLE_EXPERIMENTAL_API=True
  - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.default
  - DEPLOYMENT_ENV=ci
  - GOOGLE_APPLICATION_CREDENTIALS=/home/airflow/.config/gcloud/credentials.json
  - DATA_SCIENCE_SOURCE_DATASET=${DATA_SCIENCE_SOURCE_DATASET}
  - DATA_SCIENCE_OUTPUT_DATASET=${DATA_SCIENCE_OUTPUT_DATASET}
  - DATA_SCIENCE_STATE_PATH=${DATA_SCIENCE_STATE_PATH}


services:

  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    image: elifesciences/data-science-dags-jupyter:${IMAGE_TAG}
    command: start-notebook.sh --NotebookApp.token=''
    ports:
      - "${DATA_SCIENCE_DAGS_JUPYTER_PORT}:8888"
    volumes:
      # we need the notebooks mount to run the tests (since we don't copy them in)
      - ./notebooks:/home/jovyan/data-science-dags/notebooks
  
  data-science-pipelines:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: ${IMAGE_REPO}:${IMAGE_TAG}
    command: /bin/sh -c exit 0
    entrypoint: []

  data-science-pipelines-dev:
    environment:
      - DEPLOYMENT_ENV=ci
    build:
      context: .
      dockerfile: Dockerfile.airflow
      args:
        install_dev: y
    image:  ${IMAGE_REPO}-dev:${IMAGE_TAG}
    command: /bin/sh -c exit 0
    entrypoint: []

  test-client:
    image: ${IMAGE_REPO}-dev:${IMAGE_TAG}
    environment: *pipeline-env
    command: >
      bash -c "sudo install -D /tmp/credentials.json -m 644 -t  /home/airflow/.config/gcloud
      && sudo install -D /tmp/.aws-credentials -m 644 --no-target-directory /home/airflow/.aws/credentials
      && ./run_test.sh with-end-to-end"

  data-hub-pipelines:
      image:  ${IMAGE_REPO}:${IMAGE_TAG}
      environment: *pipeline-env

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 5
  redis:
    image: redis:5.0.5
    environment:
        - ALLOW_EMPTY_PASSWORD=yes

  peerscout-api:
    build:
      context: .
      dockerfile: Dockerfile.peerscout_api
      target: runtime
    image: elifesciences/data-science-dags_peerscout-api:${IMAGE_TAG}
    environment:
      - PEERSCOUT_API_TARGET_DATASET=ci
      - PEERSCOUT_API_URL=http://peerscout-api:8080/api/peerscout

  peerscout-api-dev:
    build:
      context: .
      dockerfile: Dockerfile.peerscout_api
      target: dev
    image: elifesciences/data-science-dags_peerscout-api_dev:${IMAGE_TAG}
    environment: 
      - PEERSCOUT_API_URL=http://peerscout-api:8080/api/peerscout
 
  wait-for-it:
    image: willwill/wait-for-it

volumes:
  data:
